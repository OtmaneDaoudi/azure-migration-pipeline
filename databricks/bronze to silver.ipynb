{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91c87c5c-e50b-42e3-88f7-3addb776e9b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "CONTAINER_NAMES = os.environ[\"container_names\"].split(\",\")\n",
    "BRONZ_CONTAINER = CONTAINER_NAMES[0]\n",
    "SILVER_CONTAINER = CONTAINER_NAMES[1]\n",
    "GOLD_CONTAINER = CONTAINER_NAMES[2]\n",
    "\n",
    "STORAGE_ACCOUNT_NAME = os.environ[\"storage_account_name\"]\n",
    "\n",
    "SAS_TOKEN = os.environ[\"sas_token\"].strip('?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8e3acc5-4ce6-404e-973d-9ea82c7fefdc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Configure access to azure data lake gen 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "060d31a9-6e18-4c5a-889c-02dcec5ea827",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(f\"fs.azure.account.auth.type.{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\", SAS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63d4d2ad-fda9-4dbb-8764-e0fe1844e22d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "BRONZE_FS = f\"abfss://{BRONZ_CONTAINER}@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n",
    "SILVER_FS = f\"abfss://{SILVER_CONTAINER}@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n",
    "GOLD_FS = f\"abfss://{GOLD_CONTAINER}@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c056054-f8f1-410e-aafb-b66f34ab02e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Transform date fields in all tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ab595e2-ae53-4296-92ad-2c4ad84240bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filenames = []\n",
    "for entry in dbutils.fs.ls(BRONZE_FS):\n",
    "  filenames.append(entry.name)\n",
    "\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b564dcc-eb92-4b8e-b5ba-add4da4161d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_utc_timestamp, date_format\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "for filename in filenames:\n",
    "  df = spark.read.format(\"parquet\").load(f\"{BRONZE_FS}/{filename}\")\n",
    "  columns = df.columns\n",
    "\n",
    "  for col in columns:\n",
    "    if \"Date\" in col or \"date\" in col:\n",
    "      df = df.withColumn(col, date_format(from_utc_timestamp(df[col].cast(TimestampType()), \"UTC\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "  df.write.format(\"delta\").mode(\"overwrite\").save(f\"{SILVER_FS}/{filename.split('.')[0]}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze to silver.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
