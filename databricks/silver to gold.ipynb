{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "CONTAINER_NAMES = os.environ[\"container_names\"].split(\",\")\n",
    "BRONZ_CONTAINER = CONTAINER_NAMES[0]\n",
    "SILVER_CONTAINER = CONTAINER_NAMES[1]\n",
    "GOLD_CONTAINER = CONTAINER_NAMES[2]\n",
    "\n",
    "STORAGE_ACCOUNT_NAME = os.environ[\"storage_account_name\"]\n",
    "\n",
    "SAS_TOKEN = os.environ[\"sas_token\"].strip('?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(f\"fs.azure.account.auth.type.{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\", SAS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRONZE_FS = f\"abfss://{BRONZ_CONTAINER}@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n",
    "SILVER_FS = f\"abfss://{SILVER_CONTAINER}@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"\n",
    "GOLD_FS = f\"abfss://{GOLD_CONTAINER}@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the list of tables to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "for entry in dbutils.fs.ls(BRONZE_FS):\n",
    "  filenames.append(entry.name)\n",
    "\n",
    "filenames"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_utc_timestamp, date_format\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "def format_column(column_name: str):\n",
    "    res = [column_name[0]]    \n",
    "    for ptr in range(1, len(column_name)):\n",
    "        if column_name[ptr] == column_name[ptr].upper() and column_name[ptr - 1] == column_name[ptr - 1].lower():\n",
    "            res.append('_')\n",
    "        res.append(column_name[ptr])\n",
    "    return ''.join(res)\n",
    "    \n",
    "\n",
    "for filename in filenames:\n",
    "  df = spark.read.format(\"delta\").load(f\"{SILVER_FS}/{filename.split('.')[0]}\")\n",
    "  columns = df.columns\n",
    "\n",
    "  for col in columns:\n",
    "    df = df.withColumnRenamed(col, format_column(col))\n",
    "\n",
    "  df.write.format(\"delta\").mode(\"overwrite\").save(f\"{GOLD_FS}/{filename.split('.')[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
